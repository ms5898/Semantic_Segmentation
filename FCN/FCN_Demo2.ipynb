{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.models.vgg import VGG\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load VOC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['background','aeroplane','bicycle','bird','boat',\n",
    "           'bottle','bus','car','cat','chair','cow','diningtable',\n",
    "           'dog','horse','motorbike','person','potted plant',\n",
    "           'sheep','sofa','train','tv/monitor']\n",
    "\n",
    "# RGB color for each class\n",
    "colormap = [[0,0,0],[128,0,0],[0,128,0], [128,128,0], [0,0,128],\n",
    "            [128,0,128],[0,128,128],[128,128,128],[64,0,0],[192,0,0],\n",
    "            [64,128,0],[192,128,0],[64,0,128],[192,0,128],\n",
    "            [64,128,128],[192,128,128],[0,64,0],[128,64,0],\n",
    "            [0,192,0],[128,192,0],[0,64,128]]\n",
    "\n",
    "class LabelToTensor(object):\n",
    "    def __call__(self, target):\n",
    "        target = np.array(target, dtype=np.float32)\n",
    "        target = np.where(target==255,0, target)\n",
    "        #target = torch.tensor(target).unsqueeze(0)\n",
    "        target = torch.tensor(target)\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './VOC'\n",
    "dataset = torchvision.datasets.VOCSegmentation(root, year='2012', image_set='train', download=False,\n",
    "                                               transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                                                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "                                              target_transform = transforms.Compose([LabelToTensor()]))\n",
    "dataset2 = torchvision.datasets.VOCSegmentation(root, year='2012', image_set='train', download=False)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1,\n",
    "                        shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 281, 500])\n",
      "torch.Size([281, 500])\n"
     ]
    }
   ],
   "source": [
    "image, target = dataset[0]\n",
    "print(image.size())\n",
    "print(target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGHead(nn.Module):\n",
    "    def __init__(self, pretrained_net):\n",
    "        super(VGGHead, self).__init__()\n",
    "        self.ranges = ((0, 5), (5, 10), (10, 17), (17, 24), (24, 31))\n",
    "        self.pretrained_net = pretrained_net\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = {}\n",
    "        # get the output of each maxpooling layer (5 maxpool in VGG net)\n",
    "        for idx in range(len(self.ranges)):\n",
    "            for layer in range(self.ranges[idx][0], self.ranges[idx][1]):\n",
    "                 x = self.pretrained_net[layer](x)\n",
    "            output[\"x%d\"%(idx+1)] = x\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN8s(nn.Module):\n",
    "    def __init__(self, pretrained_net, n_class):\n",
    "        super(FCN8s, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        self.pretrained_net = pretrained_net\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=2, stride=2, padding=0, dilation=1)\n",
    "        self.bn1 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2, padding=0, dilation=1)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2, padding=0, dilation=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2, padding=0, dilation=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2, padding=0, dilation=1)\n",
    "        self.bn5 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.pretrained_net(x)\n",
    "        \n",
    "        x5 = output['x5']  # size=(N, 512, x.H/32, x.W/32)\n",
    "        x4 = output['x4']  # size=(N, 512, x.H/16, x.W/16)\n",
    "        x3 = output['x3']  # size=(N, 256, x.H/8,  x.W/8)\n",
    "\n",
    "        score = self.relu(self.deconv1(x5))               # size=(N, 512, x.H/16, x.W/16)\n",
    "        score = F.interpolate(score, x4.size()[2:], mode='bilinear', align_corners=True)\n",
    "        score = self.bn1(score + x4)                      # element-wise add, size=(N, 512, x.H/16, x.W/16)\n",
    "        \n",
    "        score = self.relu(self.deconv2(score))            # size=(N, 256, x.H/8, x.W/8)\n",
    "        score = F.interpolate(score, x3.size()[2:], mode='bilinear', align_corners=True)\n",
    "        score = self.bn2(score + x3)                      # element-wise add, size=(N, 256, x.H/8, x.W/8)\n",
    "        \n",
    "        score = self.bn3(self.relu(self.deconv3(score)))  # size=(N, 128, x.H/4, x.W/4)\n",
    "        score = self.bn4(self.relu(self.deconv4(score)))  # size=(N, 64, x.H/2, x.W/2)\n",
    "        \n",
    "        score = self.bn5(self.relu(self.deconv5(score)))  # size=(N, 32, x.H, x.W)\n",
    "        score = self.classifier(score)                    # size=(N, n_class, x.H/1, x.W/1)\n",
    "        \n",
    "        score = F.interpolate(score, x.size()[2:], mode='bilinear', align_corners=True)\n",
    "        \n",
    "        score = score.double()\n",
    "\n",
    "        return score  # size=(N, n_class, x.H/1, x.W/1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs: FCNs-BCEWithLogits_batch1_epoch40_RMSprop_scheduler-step50-gamma0.5_lr0.0001_momentum0_w_decay0.0001\n"
     ]
    }
   ],
   "source": [
    "n_class = 21\n",
    "batch_size = 1\n",
    "epochs = 40\n",
    "lr = 1e-4\n",
    "momentum = 0\n",
    "w_decay = 1e-4\n",
    "step_size = 50\n",
    "gamma = 0.5\n",
    "\n",
    "configs    = \"FCNs-BCEWithLogits_batch{}_epoch{}_RMSprop_scheduler-step{}-gamma{}_lr{}_momentum{}_w_decay{}\".format(batch_size, \n",
    "                                                                                                                    epochs, step_size, gamma, lr, momentum, w_decay)\n",
    "print(\"Configs:\", configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish cuda loading, time elapsed 1.9775159358978271\n"
     ]
    }
   ],
   "source": [
    "# create dir for model\n",
    "model_dir = \"models\"\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "model_path = os.path.join(model_dir, configs)\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "pretrained_net = models.vgg16(pretrained=True).features\n",
    "vgghead_model = VGGHead(pretrained_net)\n",
    "fcn_model = FCN8s(pretrained_net=vgghead_model, n_class=n_class)\n",
    "\n",
    "if use_gpu:\n",
    "    ts = time.time()\n",
    "    vgghead_model = vgghead_model.cuda()\n",
    "    fcn_model = fcn_model.cuda()\n",
    "    print(\"Finish cuda loading, time elapsed {}\".format(time.time() - ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "params = [p for p in fcn_model.parameters() if p.requires_grad]\n",
    "optimizer = optim.RMSprop(params, lr=lr, momentum=momentum)\n",
    "#optimizer = optim.Adam(params, lr=lr)\n",
    "\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)  # decay LR by a factor of 0.5 every 30 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    fcn_model = fcn_model.train()\n",
    "    ts = time.time()\n",
    "    for iter, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if use_gpu:\n",
    "            inputs = Variable(batch[0].cuda())\n",
    "            labels = Variable(batch[1].cuda()).long()\n",
    "        else:\n",
    "            inputs, labels = Variable(batch[0]), Variable(batch[1]).long()\n",
    "            \n",
    "        outputs = fcn_model(inputs)\n",
    "        # outputs = outputs.argmax(axis=1).unsqueeze(0).double()  \n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if iter % 100 == 0:\n",
    "                print(\"epoch{}, iter{}, loss: {}\".format(epoch, iter, loss.item()))\n",
    "    print(\"Finish epoch {}, time elapsed {}\".format(epoch, time.time() - ts))\n",
    "    #torch.save(fcn_model, model_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(fcn_model.state_dict(), 'models/fcn_model_params.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_net2 = models.vgg16(pretrained=False).features\n",
    "vgghead_model2 = VGGHead(pretrained_net2)\n",
    "fcn_model2 = FCN8s(pretrained_net=vgghead_model2, n_class=n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcn_model2.load_state_dict(torch.load('models/fcn_model_params.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcn_model2 = fcn_model2.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './VOC_test'\n",
    "dataset_test = torchvision.datasets.VOCSegmentation(root, year='2012', image_set='val', download=False,\n",
    "                                               transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                                                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "                                              target_transform = transforms.Compose([LabelToTensor()]))\n",
    "dataset2_test = torchvision.datasets.VOCSegmentation(root, year='2012', image_set='val', download=False)\n",
    "\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=1,\n",
    "                        shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5b641f82b0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa8AAAFHCAYAAAAWfMPuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAWcklEQVR4nO3df4xlZ33f8fentjG0RLENtrXdXdVOslVxqmahW+OI/rFxoDFu1XUkUhlVYZVa2lYFCSrU1ihSM0hFSqoGV0itpaV2WSqKcYHIK4uEuotXKH9gszbGsdkQDz8ab3blFcI20EhO7Xz7x5zBl9k7O3dm7p17n3PeL+no3vPcc+8882h1Pvt9znPPpKqQJKklf2XeHZAkabMML0lScwwvSVJzDC9JUnMML0lScwwvSVJzZhZeSW5J8s0ky0nunNXPkSQNT2bxPa8klwB/ArwDOAN8FXh3VX1j6j9MkjQ4s6q8bgSWq+rbVfUXwH3AoRn9LEnSwFw6o8/dDTw7sn8GeOvoAUmOAEe63b87o35Iktr1vaq6etwLswqvjGn7ifnJqjoKHAVI4j2qJElr/Z/1XpjVtOEZYO/I/h7g7Ix+liRpYGYVXl8F9iW5PslrgNuB4zP6WZKkgZnJtGFVvZzkfcAXgUuAe6vq6Vn8LEnS8MxkqfymO+E1L0nShR6rqgPjXvAOG5Kk5hhekqTmGF6SpOYYXpKk5hhekqTmGF6SpOYYXpKk5hhekqTmGF6SpOYYXpKk5hhekqTmGF6SpOYYXpKk5hhekqTmGF6SpOYYXpKk5hhekqTmGF6SpOYYXpKk5hhekqTmGF6SpOYYXpKk5hhekqTmGF6SpOYYXpKk5hhekqTmGF6SpOYYXpKk5hhekqTmGF6SpOZcup03J/ku8EPgFeDlqjqQ5CrgM8B1wHeBf1JVz2+vm5IkvWoaldcvVdX+qjrQ7d8JnKiqfcCJbl+SpKmZxbThIeBY9/wYcNsMfoYkacC2G14F/K8kjyU50rVdW1XnALrHa8a9McmRJKeSnNpmHyRJA7Ota17A26rqbJJrgIeS/PGkb6yqo8BRgCS1zX5IkgZkW5VXVZ3tHs8DvwfcCDyXZBdA93h+u52UJGnUlsMryV9L8lOrz4F/ADwFHAcOd4cdBh7YbiclSRq1nWnDa4HfS7L6Of+jqv4gyVeB+5PcAfwp8Gvb76YkSa9K1fwvN3nNS5I0xmMjX8P6Cd5hQ5LUHMNLktQcw0uS1BzDS5LUHMNLktQcw0uS1BzDS5LUHMNLktQcw0uS1BzDa+BOLi39xPPV/dF2SVo03h5qYFZD6eBIUE3qoIEmaWd5e6ihWltZjXsuSa2x8uqpWYXTasVmFSZpB1h5DcksqyorNkmLwMqrR3Y6WKy+JM2YlZem7+QWFn1I0jQYXpKk5hhekqTmeM2rJ7Yyfbe0tLJNk9fBJE3Rute8Lt3pnmi6tnrN6SRLwNbeK0nz5rRh4w5u4ztXsyqSXMQhadacNmzcogaF04eSpsCl8tpZixqqkvrB8BoAqyBJfWN4DYBVkKS+Mbw0M96BQ9KsGF6SpOYYXg2zqpE0VIZXo1oKrpb6KqkNhleDDANJQ7dheCW5N8n5JE+NtF2V5KEkz3SPV3btSfKxJMtJnkzylll2Xu1w8YakaZqk8voEcMuatjuBE1W1DzjR7QO8E9jXbUeAu6fTTUmSXrVheFXVl4Hvr2k+BBzrnh8Dbhtp/2St+ApwRZJd0+qsJEmw9Wte11bVOYDu8ZqufTfw7MhxZ7q2CyQ5kuRUklNb7MMgtTT1tp2bBkvSxUx7wUbGtI296W5VHa2qA+vddFHjtRIG6/WzpfCVtLi2+ve8nkuyq6rOddOC57v2M8DekeP2AGe300G1yZCSNEtbrbyOA4e754eBB0ba39OtOrwJeHF1elGSpGnZsPJK8mngIPDGJGeA3wJ+G7g/yR3AnwK/1h3+BeBWYBn4c+A3ZtBnSdLA+ccoG9XytFwr1+0kzZ1/jLJvDABJQ2Z4NcwAkzRUhpd2XMtTnpIWg+GlHWfFKGm7DC/tOCsvSdu11S8pa47mdfI/yBIn2f7PtvKStF1WXprcyXl3QJJWGF6a2MGTS1P5HKcNJW2XX1JuVB8CwOlDSRtY90vKhlfDWg0wQ0vShAyvvmk1uEYZYpI24O2h+qrlAOhDAEuaD8OrB1oNsFb7LWn+DK9GtX7iP7i0xMmRTZI2w2tejVo94R/s2cm/9VCWNFXrXvPyDhuN8iQvacicNmxYnyquVU4lSpqE4dWoIZzch/A7Stoar3k1aogndqdKpcHxe16SpP5wwYaasbbatBKThsvKq1GeuIc5dSppheGlXjDIpGExvBrlyfpCjok0HK42bJQn6otzWlXqBVcbalgMd6nfDC/1lnfqkPrL8FLvGWBS/xheGgQDTOoXw0uDYYBJ/bFheCW5N8n5JE+NtC0l+bMkT3TbrSOvfSjJcpJvJvmVWXV8yDwJSxq6SSqvTwC3jGm/q6r2d9sXAJLcANwO/Hz3nv+S5JJpdVYG13a4fF7qjw3Dq6q+DHx/ws87BNxXVS9V1XeAZeDGbfRPa3gC3jqDX+qP7Vzzel+SJ7tpxSu7tt3AsyPHnOnaLpDkSJJTSU5tow+D4wl46wx+qT+2Gl53Az8L7AfOAb/btWfMsWPvnlFVR6vqwHrfnpYkaT1bCq+qeq6qXqmqvwQ+zqtTg2eAvSOH7gHObq+L0nRYtUr9saXwSrJrZPdXgdWViMeB25NcnuR6YB/w6Pa6qFWefCVpxYY35k3yaeAg8EbgOeC3uv39rEwJfhf451V1rjv+N4F/BrwMfKCqfn/DTnhj3nUZWNPntS+pGevemHfDv6RcVe8e03zPRY7/CPCRyfsmSdLmeIcNDY7VrNQ+w2uBeZKdHcdWapvhtcC8NnNxjo80XIbXArM6uLiNxsdwk/prwwUbUqvWCzdDTWqfldcC8yQ7fY6p1A+G1wIb0rThaqgYLpImseGXlHekE35J+ceGFFjzYDhKTVn3S8pWXgvGk6skbcwFGwvEqkuSJmPlpYUyy8rTqlbqDysvLZRZVJ+GltQ/Vl6SpOYYXgvC612SNDmnDReAwTUbThdK/WXlpd7yPwVSfxlekqTmOG2o3nG6UOo/Ky/1isElDYPhtQA84UrS5hhekqTmGF4Lwupr+xxDaTgMLzVvNbRcGi8Nh+GlplltScNkeC0QT8TjHVxaumBsHCtp2AwvNWM0sE6OCbTVdqcPpf4zvLTQRq9njYaS17mkYTO8Fsy4KTKNd9IAkwbL8FIT1gt0w14aplTVvPtAkvl3YgFZUWyPoSY177GqOjDuhQ0rryR7kzyc5HSSp5O8v2u/KslDSZ7pHq/s2pPkY0mWkzyZ5C3T/V0kSUM3ybThy8AHq+pNwE3Ae5PcANwJnKiqfcCJbh/gncC+bjsC3D31XkuSBm3D8Kqqc1X1ePf8h8BpYDdwCDjWHXYMuK17fgj4ZK34CnBFkl1T77m0AZfNS/21qQUbSa4D3gw8AlxbVedgJeCAa7rDdgPPjrztTNe29rOOJDmV5NTmuy1JGrKJ/xhlktcDnwM+UFU/SLLuoWPaLliQUVVHgaPdZ7tgQ5I0sYkqrySXsRJcn6qqz3fNz61OB3aP57v2M8DekbfvAc5Op7vS5jl9KPXPJKsNA9wDnK6qj468dBw43D0/DDww0v6ebtXhTcCLq9OL0ry4bF7ql0kqr7cBvw7cnOSJbrsV+G3gHUmeAd7R7QN8Afg2sAx8HPiX0+/2MHjClaTxNrzmVVV/yPjrWAC/POb4At67zX4Jv6QsSevx9lCSpOYYXgvMacPpcByl/jG8JEnNMbwkSc2Z+EvK2nlDWrCx9q8kT/PzJPWP4aWFMKSglrR9hpcWwjQrL6suqf8MLy0EKy9Jm+GCDUlScwwvLYRpTfU5ZSgNg+GluZn2CsNpfo6kxWZ4LbC+VxEGjaStMrw0N30PZ0mzY3hpbqy8JG2V4SVJao7hpd6xopP6z/CSJDXH8FpwLmqQpAsZXg3YqQAzKCW1IlU17z6QZP6daIDXcjbHMJaa91hVHRj3gpWXJKk5hpckqTmGl3rLaVapvwwvzcQiXG9ahD5Img3DqyGejDfHykvqL8OrMa0EmMEhaZYML/WaISr1k+ElSWqO4SVJao532GjULKbDRq+nTfL5q8cv6tRcK9cHJa3LO2xoOg4uLV0QCosYEovYJ0nTs2F4Jdmb5OEkp5M8neT9XftSkj9L8kS33Tryng8lWU7yzSS/MstfQPO3qJWXpP66dIJjXgY+WFWPJ/kp4LEkD3Wv3VVV/3H04CQ3ALcDPw/8deB/J/mbVfXKNDs+dIsyZTfvn7+e1X5ZgUn9tGHlVVXnqurx7vkPgdPA7ou85RBwX1W9VFXfAZaBG6fRWc3GZk7wJzd5XUySZmFT17ySXAe8GXika3pfkieT3Jvkyq5tN/DsyNvOMCbskhxJcirJqU33WlO12RAytCTN28ThleT1wOeAD1TVD4C7gZ8F9gPngN9dPXTM2y9YTVhVR6vqwHorSSRJWs8k17xIchkrwfWpqvo8QFU9N/L6x4EHu90zwN6Rt+8Bzk6lt7rArK59rTeV2ErV5bUuqd8mWW0Y4B7gdFV9dKR918hhvwo81T0/Dtye5PIk1wP7gEen12VJ0tBNMm34NuDXgZvXLIv/D0n+KMmTwC8B/wqgqp4G7ge+AfwB8F5XGs6elcarHAup/7zDRk9MYzqv9anCtQwxqXnr3mFjomteGoaTI3fPaDWwVhlcUr95eyj9WF9O+H35PSStz8qrQaNV0cGlpearJEnaLCuvBq13Y9whVxxD/t2lITK8emLad3pvIQxauLu9pNlw2rBBa6cJx92EdghTid5nURouK68eOLjmGlifXez36/vvLulVfs+rQZv5cx8bVSQX+4zVpfOtVTWGmNQb637Py/DqgdHvZ22nfaOf0RIDTOoFw0vbtwgBNsldQAwuqTfWDS+veWlTDm6hapsFF2tIw2bl1WNbmR6c5LNGr7ktanAsQsBK2jYrryGa5gl8vRWNOx0Sayu/RakEJe0sv+elLZtHBTbu5yxq9Sdpdpw21NTMOkQmWdYvqVecNtTsjZtOnEagTDI1aHBJw2J4aarWBphTepJmwfDS1G21Chr3PisqSeN4zUsLxy8cS+p4zUvzMcm0oVOLkjbL8NJMWTlJmgWnDTVzW62s1rup8MVel9Qr604b+iVlzdza20pNcqwkXYyVl3bceiG2meDyS8nSIFh5abFNGkQu7pAEQFXNfQPKbbjbyaWlHXmPm5tbc9up9XLD1YaaO6f/JG2W4aWmnJziPRMltcsFG5KkReUdNiRJ/bFheCV5bZJHk3w9ydNJPty1X5/kkSTPJPlMktd07Zd3+8vd69fN9leQJA3NJJXXS8DNVfULwH7gliQ3Ab8D3FVV+4DngTu64+8Anq+qnwPu6o6Tps5l89KAbXJJ+18FHgfeCnwPuLRr/0Xgi93zLwK/2D2/tDsuLpV3c3Nzc9vktr2l8kkuSfIEcB54CPgW8EJVvdwdcgbY3T3fDTwL0L3+IvCGMZ95JMmpJKcm6YMkSasmCq+qeqWq9gN7gBuBN407rHvMRV4b/cyjVXVgvZUkkiStZ1OrDavqBeAkcBNwRZLV20vtAc52z88AewG6138a+P40OitJEky22vDqJFd0z18HvB04DTwMvKs77DDwQPf8eLdP9/qXahG+TCZJ6o1Jbsy7CziW5BJWwu7+qnowyTeA+5L8e+BrwD3d8fcA/z3JMisV1+0z6LckacC8w4YkaVF5hw1JUn8YXpKk5hhekqTmGF6SpOYYXpKk5hhekqTmGF6SpOYYXpKk5hhekqTmGF6SpOYYXpKk5hhekqTmGF6SpOYYXpKk5hhekqTmGF6SpOYYXpKk5hhekqTmGF6SpOYYXpKk5hhekqTmGF6SpOYYXpKk5hhekqTmGF6SpOYYXpKk5hhekqTmGF6SpOYYXpKk5hhekqTmbBheSV6b5NEkX0/ydJIPd+2fSPKdJE902/6uPUk+lmQ5yZNJ3jLrX0KSNCyXTnDMS8DNVfWjJJcBf5jk97vX/nVVfXbN8e8E9nXbW4G7u0dJkqZiw8qrVvyo272s2+oibzkEfLJ731eAK5Ls2n5XJUlaMdE1rySXJHkCOA88VFWPdC99pJsavCvJ5V3bbuDZkbef6drWfuaRJKeSnNpG/yVJAzRReFXVK1W1H9gD3JjkbwMfAv4W8PeAq4B/2x2ecR8x5jOPVtWBqjqwpZ5LkgZrU6sNq+oF4CRwS1Wd66YGXwL+G3Bjd9gZYO/I2/YAZ6fQV0mSgMlWG16d5Iru+euAtwN/vHodK0mA24CnurccB97TrTq8CXixqs7NpPeSpEGaZLXhLuBYkktYCbv7q+rBJF9KcjUr04RPAP+iO/4LwK3AMvDnwG9Mv9uSpCFL1cUWDu5QJ5L5d0KStGgeW29dhHfYkCQ1x/CSJDXH8JIkNcfwkiQ1x/CSJDXH8JIkNcfwkiQ1x/CSJDXH8JIkNcfwkiQ1x/CSJDXH8JIkNcfwkiQ1Z5I/ibITvgf83+5Rm/NGHLetcNy2xnHbGsdta/7Gei8sxJ9EAUhyar1b32t9jtvWOG5b47htjeM2fU4bSpKaY3hJkpqzSOF1dN4daJTjtjWO29Y4blvjuE3ZwlzzkiRpUotUeUmSNBHDS5LUnLmHV5JbknwzyXKSO+fdn0WS5N4k55M8NdJ2VZKHkjzTPV7ZtSfJx7pxfDLJW+bX8/lKsjfJw0lOJ3k6yfu7dsfuIpK8NsmjSb7ejduHu/brkzzSjdtnkryma7+821/uXr9unv2ftySXJPlakge7fcdthuYaXkkuAf4z8E7gBuDdSW6YZ58WzCeAW9a03QmcqKp9wIluH1bGcF+3HQHu3qE+LqKXgQ9W1ZuAm4D3dv+uHLuLewm4uap+AdgP3JLkJuB3gLu6cXseuKM7/g7g+ar6OeCu7rghez9wemTfcZuheVdeNwLLVfXtqvoL4D7g0Jz7tDCq6svA99c0HwKOdc+PAbeNtH+yVnwFuCLJrp3p6WKpqnNV9Xj3/IesnFB249hdVPf7/6jbvazbCrgZ+GzXvnbcVsfzs8AvJ8kOdXehJNkD/EPgv3b7wXGbqXmH127g2ZH9M12b1ndtVZ2DlZM0cE3X7liO0U3JvBl4BMduQ93U1xPAeeAh4FvAC1X1cnfI6Nj8eNy6118E3rCzPV4Y/wn4N8BfdvtvwHGbqXmH17j/bbh2f2scyzWSvB74HPCBqvrBxQ4d0zbIsauqV6pqP7CHlZmRN407rHt03IAk/wg4X1WPjTaPOdRxm6J5h9cZYO/I/h7g7Jz60ornVqe0usfzXbtjOSLJZawE16eq6vNds2M3oap6ATjJyjXDK5Ks3sR7dGx+PG7d6z/NhdPcQ/A24B8n+S4rlz5uZqUSc9xmaN7h9VVgX7cq5zXA7cDxOfdp0R0HDnfPDwMPjLS/p1s5dxPw4uoU2dB01w/uAU5X1UdHXnLsLiLJ1Umu6J6/Dng7K9cLHwbe1R22dtxWx/NdwJdqgHc9qKoPVdWeqrqOlXPYl6rqn+K4zVZVzXUDbgX+hJW59d+cd38WaQM+DZwD/h8r/1u7g5W58RPAM93jVd2xYWXl5reAPwIOzLv/cxy3v8/KNMyTwBPddqtjt+G4/R3ga924PQX8u679Z4BHgWXgfwKXd+2v7faXu9d/Zt6/w7w34CDwoOM2+83bQ0mSmjPvaUNJkjbN8JIkNcfwkiQ1x/CSJDXH8JIkNcfwkiQ1x/CSJDXn/wPjEI1+/XJcQAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "process = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "image2, target2 = dataset2[400]\n",
    "image2 = process(image2)\n",
    "\n",
    "fcn_model2.eval()\n",
    "fcn_model.eval()\n",
    "cm = np.array(colormap).astype('uint8')\n",
    "im = Variable(image2.unsqueeze(0)).cuda()\n",
    "out = fcn_model2(im)\n",
    "pred = out.max(1)[1].squeeze().cpu().data.numpy()\n",
    "pred = cm[pred]\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfQAAAF3CAMAAABkLEnOAAADAFBMVEUAAACAAAAAgACAgAAAAICAAIAAgICAgIBAAADAAABAgADAgABAAIDAAIBAgIDAgIAAQACAQAAAwACAwAAAQICAQIAAwICAwIBAQADAQABAwADAwABAQIDAQIBAwIDAwIAAAECAAEAAgECAgEAAAMCAAMAAgMCAgMBAAEDAAEBAgEDAgEBAAMDAAMBAgMDAgMAAQECAQEAAwECAwEAAQMCAQMAAwMCAwMBAQEDAQEBAwEDAwEBAQMDAQMBAwMDAwMAgAACgAAAggACggAAgAICgAIAggICggIBgAADgAABggADggABgAIDgAIBggIDggIAgQACgQAAgwACgwAAgQICgQIAgwICgwIBgQADgQABgwADgwABgQIDgQIBgwIDgwIAgAECgAEAggECggEAgAMCgAMAggMCggMBgAEDgAEBggEDggEBgAMDgAMBggMDggMAgQECgQEAgwECgwEAgQMCgQMAgwMCgwMBgQEDgQEBgwEDgwEBgQMDgQMBgwMDgwMAAIACAIAAAoACAoAAAIICAIIAAoICAoIBAIADAIABAoADAoABAIIDAIIBAoIDAoIAAYACAYAAA4ACA4AAAYICAYIAA4ICA4IBAYADAYABA4ADA4ABAYIDAYIBA4IDA4IAAIECAIEAAoECAoEAAIMCAIMAAoMCAoMBAIEDAIEBAoEDAoEBAIMDAIMBAoMDAoMAAYECAYEAA4ECA4EAAYMCAYMAA4MCA4MBAYEDAYEBA4EDA4EBAYMDAYMBA4MDA4MAgIACgIAAgoACgoAAgIICgIIAgoICgoIBgIADgIABgoADgoABgIIDgIIBgoIDgoIAgYACgYAAg4ACg4AAgYICgYIAg4ICg4IBgYADgYABg4ADg4ABgYIDgYIBg4IDg4IAgIECgIEAgoECgoEAgIMCgIMAgoMCgoMBgIEDgIEBgoEDgoEBgIMDgIMBgoMDgoMAgYECgYEAg4ECg4EAgYMCgYMAg4MCg4MBgYEDgYEBg4EDg4EBgYMDgYMBg4MDg4MCa7rFGAAAE5klEQVR4nO3cC3LjOAxAwbmB739aTW1l8ttYsWVRAEh0n0CsF8Ii7cqfPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALCu7T/ZD0GM7Zvsp+FS233Zj8UVdmKrvqxfc99UX9FO7A+qr2c3turr2o+t+rIeFVd9PQ+3uerreS666it5srnqK3k6uurreLq56st4fqO/V89+Yk47FF31NRxqbsAv4dhGV30JR6OrvoCjzVWf3+GNfvMyN71Xoqs+uVeaG/Bze2mjqz63F6Mb8DN7sbnqE3t1oxvwE3s9uq0+rdebv1XPfn5ecDa66vM5Md1Fn9Wp6Ob7nE41t9WndG6jiz6lk9FVn9HJ5qLP6Gx01SckekOno6s+n0HRVZ/J+ei2+nQGRHctNxvRGzp7OXMz3ucjekdjomevgkNEb0j0hkRvyOV7Q6I3NCR69iI4RvR+zl/OiD4d0RtyYmvI1XtDo345I/tEhkWXfR4Do8s+ixG/nLnJPpcBP6L4lj17PTxhUHTZpzIq+mf27BXx0LCtLvs8RkY342cxMrrskxgb3fFtCoOjyz6DoR/qss/hguiyl3dFdNmLuya67KVdFV32wq6L7the1pXRZS/q2uhu5Eu6Orqf0dVzyTn9bvXslfIhIrofStcS01z0UoKi30SvI6q56IWERle9hLDmotcRF918ryKwuehFbNHRVc8X2Vz0GkKbm+81JERXPVlwc9EriI5uvucLb26rpws9roleQkJz8z1ZSnPRc6U0Fz1VXnPRs+QMd9FTad5PTnPRMyUNd69xmWz0fpKai57JRu8ns7noSRKPa5on0byhtC9aNE+jeUNZ36JrnkjzhuKja54ufLxrni86uuYVxFbXvITA6NumeRHx1bNXTPj/IVC9AtUb8irXkeoNBZ/VRS8h4YIme8nY6g2J3lFC9ewlI3pH5ntDjuoNZURXPVvGC7zqyYKj+1gvIaN69prbE70j0RtyVG9I9IbcxDYkekOiNyR6Q/G379krJjS6q/ciAqNrXkV09Oz18icyuuZlhEU33OuIiq55IUHRNa8kMnr2WvknJrrmpYREN9xriYuevVI+hPxIzk6vJSS66sV4k2so8MyWvVTeid6Q6A2J3pB72IZEbyjyu9XstfKP6A2J3pDoDcVcvoteyhZU3et7JUHVRS8lsHr2UvkQU130WkTvKKK6D/ViIga86NVEbfXsdfKV6A2Z7w2J3pH53pDoDQXN9+xl8pXoHYne0PVbXfRyRO/o8uqi1yN6R3vRR30HJ3pBO223bUx117AVPYh+svomekl3y27bkOqaF3W37Fuss9E1r+pe2fdYI6Jnr497fpbdhkS3zwv7sdU/a70+3z9eCbJXx13/L/sl1sHo20/Ja2PP97JfY52Onrsw9u1v0JPRU1fFr/Y36MGPdMUnsrtDX4meuA6O2JvKB6PfRF+A6A2J3pDoDYne0PbCQT37mTnp841e9D6+HeVEb+JYddEXcaS66At5trroixG9oyeqi76aJwa86Mt5XF309TysLvqCHlUXfUW/Rn/7k8h+RIbbr/5+lM9+QobbHfCaL+x+9U3zpf24kf1yN5/9bFxl25P9YFxI844k70lwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACDHX6DBV4yk9LJ2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=P size=500x375 at 0x7F5B6424B630>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 366, 500])\n",
      "torch.Size([366, 500])\n"
     ]
    }
   ],
   "source": [
    "test_image, test_target = dataset_test[0]\n",
    "test_image2, test_target2 = dataset2_test[0]\n",
    "print(test_image.size())\n",
    "print(test_target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(pred, target):\n",
    "    # 输入都是1维度\n",
    "    ious = []\n",
    "    for cls in range(n_class):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = pred_inds[target_inds].sum()\n",
    "        union = pred_inds.sum() + target_inds.sum() - intersection\n",
    "        if union == 0:\n",
    "            ious.append(float('nan'))  # if there is no ground truth, do not include in evaluation\n",
    "        else:\n",
    "            ious.append(float(intersection) / max(union, 1))\n",
    "        # print(\"cls\", cls, pred_inds.sum(), target_inds.sum(), intersection, float(intersection) / max(union, 1))\n",
    "    return ious\n",
    "\n",
    "def pixel_acc(pred, target):\n",
    "    correct = (pred == target).sum()\n",
    "    total   = (target == target).sum()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pix_acc: 0.945934372913235, meanIoU: 0.11417843944312747\n"
     ]
    }
   ],
   "source": [
    "total_ious = []\n",
    "pixel_accs = []\n",
    "for iter, batch in enumerate(dataloader):\n",
    "    if use_gpu:\n",
    "        inputs = Variable(batch[0].cuda())\n",
    "        labels = Variable(batch[1].cuda()).long()\n",
    "    else:\n",
    "        inputs, labels = Variable(batch[0]), Variable(batch[1]).long()\n",
    "    \n",
    "    outputs = fcn_model2(inputs)\n",
    "    output = outputs.data.cpu().numpy()  \n",
    "    N, _, h, w = output.shape\n",
    "    pred = output.transpose(0, 2, 3, 1).reshape(-1, n_class).argmax(axis=1).reshape(N, h, w)\n",
    "    target = labels.cpu().numpy().reshape(N, h, w)\n",
    "    \n",
    "    total_ious.append(iou(pred, target))\n",
    "    pixel_accs.append(pixel_acc(pred, target))\n",
    "    if iter == 7:\n",
    "        break;\n",
    "# Calculate average IoU\n",
    "total_ious = np.array(total_ious).T  # n_class * val_len\n",
    "ious = np.nanmean(total_ious, axis=1)\n",
    "pixel_accs = np.array(pixel_accs).mean()\n",
    "print(\"pix_acc: {}, meanIoU: {}\".format(pixel_accs, np.nanmean(ious)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
